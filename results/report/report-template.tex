
\title{\polystat{} vs. Others: \texorpdfstring{\\}{} Static Analysis Tools Benchmark Report}
\author{\polystat{} Team \texorpdfstring{\\}{} \today}% <-this % stops a space

\begin{abstract}
This document presents a preliminary report on the performance of \polystat{} compared with other static analysis tool across several types of defects in object-oriented programs.
\end{abstract}

\maketitle
\pagestyle{plain}

\section{Introduction}

\polystat{} is a static analyzer for object-oriented languages, based on $\varphi$-calculus. The calculus and EO programming language have been introduced by \citet{eolang-paper} in an attempt to distill his vision of the essence of object-oriented programming. EO is a very minimalistic programming language and is argued to be capable of representing faithfully many object-oriented structures commonly occurring in modern software engineering.

At the time of writing, \polystat{} consists of two main parts:
\begin{inparaenum}[a)]
\item \ff{polystat} itself offering the primary entry point and the module for division-by-zero defect detection, and
\item \ff{odin} module for object-related defect detection.
\end{inparaenum}
\polystat{} is capable of analyzing only EO programs.

For comparison with \polystat{}, we use popular open-source analyzers that are capable to detect defects in C++ programs: Clang-Tidy, Cppcheck, and SVF. In this document, we compare analyzers using several static analysis metrics and interpret the benchmark report.

As a result of the research we have established that:
\begin{inparaenum}[a)]
    \item among the analyzers participating in the comparison, only \polystat{} can find OOP-specific defects,
    \item \polystat{} shows 100\% recall for these OOP-specific defects,
    and
    \item \polystat{} shows comparable performance for the common type of defects.
\end{inparaenum}
\Cref{tab:metrics} demonstrates the numbers collected.

#summary_table

\section{Materials and Methods}

In this section we describe our approach to benchmarking static analysis tools for C++ and EO. We first describe the general methodology. Then, we present a method of translating programs from C++ to EO. Finally, we give descriptions of three types of defects that are of special interest in this report.

\subsection{Comparing static analysis tools}

For this report, we use a direct and simple approach of comparing static analysis tools. Put simply, we have a collection of example programs marked as ``good'' (meaning that the program is defect-free) or ``bad'' (meaning that it has some defect). We run static analysis tools on these programs and check whether the tool agrees with the markings.

The approach has several limitations, such as ignoring actual type, location, and confidence level of defect reported by the tool, as well as supporting programs with multiple defects. However, for preliminary comparison, this approach works well.

To organize the comparison, we collect a suite of test files for each type of defects. Each test file is targeting a specific circumstances of the defect. For instance, division by zero may be harder to detect for some tools when numeric type casting is involved, so we may add a test file for that scenario.

Each test file, for each supported programming language, presents two similar versions of a program---one good and one bad. The versions of the programs can be thought of as ``before'' and ``after'' fixing the corresponding defect. The versions in different languages are expected to be equivalent, at least from a software engineer's perspective.

We implement test files as YAML documents with the following structure:

\begin{ffcode}
title: # Title
description: >
  # Detailed description
features: # a list of tags
bad:
  source.cpp: |
    # bad C++ program
  test.eo: |
    # bad EO program
good:
  source.cpp: |
    # good C++ program
  test.eo: |
    # good EO program
\end{ffcode}

They are then used by automatic continuous integration scripts, to evaluate static analyzers whenever the benchmark suite repository on GitHub is updated.

\subsection{Metrics}\label{sec:metrics}

Assuming each tool is evaluated using one programming language, every test file contains essentially two programs. Running a tool on an a program leads to one of the following possible outcomes: True Positive~(TP), False Positive~(FP), True Negative~(TN), False Negative~(FN), or Error~(ERR). Evaluating each tool on a collection of test files, we accumulate the following metrics:

Total count per type of outcome:
\begin{equation*}
\text{Total} = TP + TN + FP + FN + ERR.
\end{equation*}

``Accuracy'' as a ratio of TP+TN outcomes to the total number of test programs; this metric helps understand how good a tool is at predicting the presence of a defect:
\begin{equation*}
\text{Accuracy} = \frac{TP + TN}{\text{Total}}.
\end{equation*}

``Precision'' as a ratio of TP to the total number of TP+FP outcomes (predicted positives); this metric helps us understand how ``useful'' are positive detections of a defect in a program by a tool:
\begin{equation*}
\text{Precision} = \frac{TP}{TP + FP}.
\end{equation*}

``Recall'' as a ratio of TP to the total number of TP+FN programs (actual positives); this metric helps us understand how well are actual defects detected by a tool:
\begin{equation*}
\text{Recall} = \frac{TP}{TP + FN}.
\end{equation*}

``F1 Score'' as a harmonic mean of Precision and Recall; this metric is commonly used for preliminary comparison of tools, as high F1 score indicates that both Precision and Recall are good:
\begin{equation*}
F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}.
\end{equation*}

Executing the overall benchmark produces an automated report, consisting of three parts. All metrics are grouped by the type of defect and the tool, forming the ``statistics table.'' A detailed account of specific output for each tool run of every test file is recorded in a ``details table.'' In this report we present only the statistics table, leaving the detailed parts of the report in the Appendix.


\subsection{From C++ to EO}

To properly compare \polystat{} with other analyzers, we need a way to provide equivalent programs in C++ and EO. While \ff{c2eo}\footnote{For more details see \url{https://github.com/polystat/c2eo}}, a C/C++ to EO translator, is under development, it is not ready to be used for the purposes of this benchmark. In particular, many code examples used in our test files cannot be translated by the tool at the moment of writing.

To facilitate comparison, we translate C++ programs to EO manually. Some translation techniques have been discussed by \citet{bugayenko2021reducing} and \citet[Section 2]{KudasovShilovStepanov2021}. We apply those techniques manually, following these general principles:
\begin{inparaenum}[a)]
    \item classes are translated to objects, capable of generating new objects;
    \item inheritance is approximated with decoration;
    \item \ff{return} value is assigned to \ff{@} attribute of a method.
\end{inparaenum}

While manual translation has its limitations, we believe that our test files are representative of \polystat{}'s desired capabilities.

\subsection{Types of defects}

In this report, we focus on four types of defects. The first one, ``division by zero,'' is a basic type of defect supported by many tools. The other three are OOP-specific defects belonging to a category of fragile base class problems~\citep{MikhajlovSekerinski1998}.

\begin{itemize}
\item \ff{division-by-zero}: For this type of defect, we are looking for erroneous or at least fragile code, that may lead to a divide by zero error. \polystat{} detects this defect using an approach called ``find-a-reverse'' described by \citet{eolang-far}.
Clang-Tidy supports detection of this defect via Clang Static Analyzer module \ff{core.DivideZero}\footnote{For more details see \url{https://clang.llvm.org/docs/analyzer/checkers.html}}.
Support for this defect is also mentioned in Cppcheck, among other\footnote{For more details see \url{https://sourceforge.net/p/cppcheck/wiki/ListOfChecks/}} defects.

\item \ff{mutual-recursion}: Here, we are interested in mutual recursion that occurs as a result of dynamic dispatch (virtual method overriding in subclasses). This problem is described for class-based OOP languages by \citet[Section 3.1]{MikhajlovSekerinski1998} and for elegant objects by \citet[Section 3.2]{KudasovShilovStepanov2021}. \polystat{} supports this type of defect via \ff{odin} module. Other analyzers do not claim to support this.

\item \ff{unjustified-assumption}: Here, we are interested in unjustified assumptions that occurs as a result of dynamic dispatch (virtual method overriding in subclasses). This problem is described for class-based OOP languages by \citet[Section 3.3]{MikhajlovSekerinski1998} and for elegant objects by \citet{KudasovSim2022}. \polystat{} supports this type of defect via \texttt{odin} module. Other analyzers do not claim to support this.

\item \ff{direct-state-access}: Here, we are interested in access (read/write) to the base class state without getters and setters methods. This problem is described for class-based OOP languages by \citet[Section 3.4]{MikhajlovSekerinski1998}. \polystat{} supports this type of defect via \ff{odin} module. Other analyzers do not claim to support this.
\end{itemize}

In future versions of \polystat{} more types of bugs will be detectable.

\subsection{Analyzers for comparison}

The main criteria for choosing analyzers for comparison with \polystat{} were the availability and ease of integration. The selected analyzers are briefly described below:

\begin{itemize}
    \item \textbf{Clang-Tidy} is a clang-based tool that is capable to detect various defects in C++ programs. In particular, Clang-Tidy includes C and C++ checkers from Clang Static Analyzer project.
    \item \textbf{Cppcheck} is an open source static code analysis tool for the C and C++ programming languages. It is versatile, and can check non-standard code including various compiler extensions, inline  assembly code, etc. Its internal preprocessor can handle includes, macros, and several preprocessor commands.
    \item \textbf{SVF} is a source code analysis tool that enables interprocedural dependence analysis for LLVM-based languages. SVF is able to perform pointer alias analysis, memory SSA form construction, value-flow tracking for program variables and memory error checking.
\end{itemize}

If future reports we may use more analyzers.

\section{Results}
Benchmarking was carried out on a set of #number_of_tests tests. In the statistic table in the appendix, we can see a summary comparison of \polystat{} and other analyzer performance over four types of defects.

Our interpretation of the metrics is as follows:
\begin{enumerate}
    \item For OOP-specific defects, only \polystat{} has TP results. That says about his ability to find such defects.
    \item Analyzers that did not claim to support OOP-specific defects, given the absence of TP results, do not actually find them. However, since they do not have FPs for those cases, the accuracy is exactly 50\%.
    \item \polystat{} shows 100\% Recall for the OOP-specific defects on our test suit, meaning that \polystat{} has successfully detected all bad programs in test files. This indicates its good ability to find such defects, however, because of some FNs, the overall Accuracy is not as high. The FNs come from programs with branching (such as \ff{if} statements or \ff{while} loops), and in its current form \polystat{} cannot properly understand whether the condition should be taken into account.
    \item For the division-by-zero defect that is not specific to OOP programs, the Clang-Tidy showed the highest Accuracy of #c_div_by_zero\%. Polistat is only #c_p_div_by_zero\% behind.
\end{enumerate}

Overall, we see that \polystat{} shows comparable performance for the common type of defects (division by zero), while the detection of OOP-related defects among the considered analyzers is supported only by \polystat{}.

\section{Discussion and Summary}

Our approach to benchmarking static analyzers has several limitations that we would like to address in the future work.

First, we would like to understand better, whether the tool correctly recognizes the type and location of the defect. However, this is technically difficult. In particular, we tried the following approaches:
\begin{enumerate}
    \item Checking the reported line at which the defect was detected. Here, different analyzer may point out the defect at different lines in the source program.
    \item Checking the pattern of defect messages. Unfortunately, the actual messages are not standardized, so extra analysis is required to  compare output of different analyzer tools;
    \item Check the defect/error code of the tool. Some analyzers do not report defect codes. Some analyzers use their internal code system, which requires mapping to some standardized map.
\end{enumerate}

We also notice that while some of the defects (such as division by zero, or specific security vulnerabilities) are more or less standard across static analysis tools, OOP-specific defects are not standardized. To improve benchmarks for OOP static analysis, we suggest analysis and standardization of the common defects and anti-patterns in OOP programs.

Our benchmark test suit allows comparing static analyzers in a few specific scenarios. However, for the sake of completeness we think it is important to expand the test set in the future.

\section{Conclusion}

In this report, we have presented a basic methodology for comparing static analysis tools, a translation approach for comparing C++ tools with EO tools, and benchmark results comparing \polystat{} and other popular open source analyzers over test suit covering four types of defects: division by zero, unanticipated mutual recursion in subclasses, unjustified assumptions in subclasses, and direct access to the base class state.

We have shown that \polystat{} offers comparable performance for basic defects like division by zero, and outperforms others for detecting OOP-specific defects.

\printbibliography

\onecolumn
\appendix
